

**** How do you approach writing and maintaining automation scripts **** How you would develop and execute an automated test script 
 *** How do you ensure the reliability and reusability of your automated test scripts

1. Use a Robust Framework --> Follow best practices such as the Page Object Model (POM) or Design pattern to separate test logic from UI interaction.
2. Write Clean and Maintainable Code --> Use meaningful names for classes, methods, and variables or clean coding principles (e.g.SOLID Princiaple).
3. Parameterization and Data-Driven Testing --> Use external files (e.g., JSON, Excel, or CSV) to supply test data, enabling easy updates without modifying scripts.
4. Dynamic Locators and hadnling Synchronization --> Write locators that adapt to changes in the DOM (e.g., XPath, CSS selectors). ,Use auto-wait mechanisms to handle dynamic elements.
5. Comprehensive Error Handling --> Add try-catch blocks for critical operations. ,Implement assertions with meaningful error messages.
6. Version Control -->Use Git for versioning scripts and collaboration, Maintain a branch strategy for test scripts (e.g., development and release branches).
7. Cross-Browser and Platform Testing --> Test scripts on multiple browsers and devices using tools like Selenium Grid or BrowserStack.and CI/CD
8. Regular Maintenance --> Refactor scripts periodically to remove redundancy and improve performance or Code Smell useing Sonar Cube tools
9. Reporting and Alerts --> Use reporting tools (e.g., Allure, Extent Reports).


***************** Key challeges in selenium automation *************
1. Dynamic Web Elements
	Challenge: Modern web applications frequently use dynamic elements, where IDs, class names, 
	           or other attributes change on each reload or session.
	Solution: Use dynamic locators like XPath, CSS selectors, or attribute-based strategies (e.g., contains()
	           or starts-with() in XPath).
			   
2. Handling Asynchronous Elements
	Challenge: Web elements might not be available immediately due to delays like AJAX calls or animations.
	Solution: Use explicit waits (WebDriverWait) to ensure elements are interactable before proceeding.
	
3. Cross-Browser Compatibility
	Challenge: Tests may behave differently across browsers due to variations in rendering engines.
	Solution: Regularly test across multiple browsers and utilize cloud platforms like BrowserStack or 
	Sauce Labs for wider coverage.
	
4. Pop-Ups and Alerts
	Challenge: Managing unexpected pop-ups, system alerts, or modal dialogs.
	Solution: Handle pop-ups using Selenium's Alert interface and design test flows to anticipate such occurrences.
	
5. Captcha and Two-Factor Authentication (2FA)
	Challenge: These security measures prevent automated interactions.
	Solution: Use pre-configured test environments where these features are disabled, or mock backend
	responses where feasible.
	
6. Managing Test Data
	Challenge: Tests often require consistent, repeatable data, which can be difficult in dynamic systems.
	Solution: Use databases, API calls, or pre-set test configurations to ensure consistent data availability.
	
7. Flaky Tests
	Challenge: Tests may intermittently fail due to timing issues, dependencies, or environmental changes.	
	Solution: Analyze root causes, stabilize locators, introduce appropriate waits, and reduce inter-test dependencies.
	
8. Scalability and Maintenance
	Challenge: As test suites grow, maintaining locators and test logic becomes cumbersome.
	Solution: Implement Page Object Model (POM), modularize test scripts, and use version control systems.
	
9. Performance Testing
	Challenge: Selenium is not designed for performance or load testing.
	Solution: Use dedicated tools like JMeter, Gatling, or BlazeMeter for performance testing while using 
	Selenium for functional checks.
	
10. Environmental Dependencies
	Challenge: Test environments may differ from production, leading to false positives or negatives.
	Solution: Standardize environments using containers (e.g., Docker) or virtualization.
	
*********** By using webdriver what are the UI complications you faced ************************
Using WebDriver for UI automation comes with several challenges, primarily due to the dynamic and complex 
nature of modern web applications.

1. Element Identification Issues
Dynamic Elements: Web elements with frequently changing attributes make them hard to locate reliably.
Hidden Elements: Some elements may not be visible or interactable causing ElementNotVisibleException.
Overlapping Elements: Other elements might obstruct interaction with the intended element.
Solution: Use robust locators like XPath, CSS selectors, or a combination of stable attributes (e.g., data-testid) 
and wait strategies like WebDriverWait.

2. Synchronization Issues
Page Load Delays: Actions may fail if the page or elements are not fully loaded.
Animations: Transition effects may cause timing issues, leading to flaky tests.
Solution: Use explicit waits (e.g., WebDriverWait with conditions like elementToBeClickable) or fluent waits 
instead of relying on static delays.

3. Handling Frames and Windows
iFrames: Interacting with elements inside an <iframe> requires switching the context to the frame, which is 
often overlooked.
Multiple Windows: Managing multiple browser tabs or windows can complicate element interaction.
Solution: Use driver.switchTo().frame() for iFrames and driver.switchTo().window() for handling windows.

4. Browser Compatibility Issues
Tests that work on one browser might fail on another due to rendering differences, JavaScript execution timing, 
or browser-specific behaviors.
Solution: Run cross-browser testing using tools like Selenium Grid or services like BrowserStack/Sauce Labs.

5. Handling Pop-Ups and Alerts
JavaScript Alerts: Unexpected alerts might disrupt test execution.
Custom Pop-Ups: Custom modal dialogs can be hard to detect and handle.
Solution: Use driver.switchTo().alert() for JavaScript alerts and proper locators for custom pop-ups.

6. Dynamic Content and Asynchronous Events
Dynamic updates, like content loading via AJAX or web sockets, may lead to tests interacting with incomplete 
or stale data.
Solution: Use WebDriverWait to wait for specific conditions, such as visibility of elements or absence of loaders.

7. Flaky Tests
Minor UI changes, network instability, or timing issues can cause tests to pass sometimes and fail other times.
Solution: Improve test design by reducing reliance on exact UI structure and leveraging reusable utility methods.

8. File Uploads and Downloads
Handling file inputs (e.g., upload buttons) and verifying downloaded files can be tricky as WebDriver cannot 
directly interact with OS-level dialogs.

9. Responsive Design and Mobile Testing
Verifying UI on different screen resolutions can be challenging, especially for mobile views.
Solution: Use tools like Browser DevTools, emulators, or frameworks like Appium for responsive testing.

10. Performance and Scalability
UI tests with WebDriver can be slow due to browser interactions, especially for large test suites.
Solution: Run tests in parallel using frameworks like TestNG or JUnit, and focus on optimizing locators 
and test workflows.

************** Which one is better backend or UI testing ******************
Choose backend automation if you value stability, speed, and deeper technical challenges.
 Opt for UI automation if you're inclined towards end-user experience and testing the visual aspects of 
 applications. Both roles are crucial, and combining them often provides the best coverage in a testing strategy.
 
Backend Automation Testing:
	Focus: Testing APIs, databases, business logic, and server-side operations.
	Pros:
		Faster test execution and feedback.
		Easier to maintain because backend code is less likely to change frequently compared to UI.
		Better for testing critical business logic.
		Requires less setup for automation frameworks.
		Skills Needed: Strong knowledge of APIs, databases, and tools like Postman, RestAssured, or SOAPUI; programming languages like Java, Python, or JavaScript.
		Best For: Performance testing, integration testing, and verifying business logic.

UI Automation Testing
	Focus: Testing the graphical user interface, including user workflows and frontend behavior.
	Pros:
		Validates user experience, ensuring the UI behaves as expected.
		Directly impacts end-user satisfaction.
		Identifies visual and usability issues.
		Skills Needed: Familiarity with UI frameworks like Selenium or Playwright; CSS, HTML, and JavaScript basics.
	Cons:
		Slower test execution.
		More prone to breakage due to frequent UI changes.

********************** what is entry and exit criteria for manual test cases ****************
Entry Criteria -> Conditions that must be met before a testing phase or process begins., ensures testing can start without interruptions.
Exit  Criteria -> Conditions that must be satisfied to conclude a testing phase or process, objectives are met and results are acceptable for project delivery or the next phase.

Manual Testing:
Entry Criteria -> Requirements Clarity, Test Plan and Test Cases, Test Environment, Build Availability, Test Data,Defect Management Tool
Exit  Criteria -> Test Case Execution, Defect Resolution,Test Results Documentation,Requirement Coverage,Sign-Off

Automation Testing:
Entry Criteria ->  Stable Application Build, Automation Framework ,Automation Tools ,Test Environment ,Script Readiness
Exit  Criteria -> Script Execution Completion,Defect Verification,Test Coverage,Regression Results,Reports Generation,Framework Stability,Stakeholder Approval

******************* what are your roles and responsibility as an automation tester **********************
As an automation tester I participated in meetings like daily stand-ups, sprint reviews, and retrospectives to stay 
aligned with team goals.
In Sprint planning meeting we define testing scope, goal and strategy which will be required for testing.
In Daily Stand-up (Daily Scrum) : We discuss about the status of each insprint jira with 
development as well as testing perspective.
In Sprint Review meeting: We demonstrate about the work completed during the sprint and gather feedback.
In Sprint Retrospective meeting: We were discussing about Pros and Cons which happened during the sprint.
what all things we can make better in next sprint.


*******************************************  process of automating API testing using tools like Postman or Rest Assured ********************

Understand API Specifications
Action: Review API documentation (e.g., Swagger/OpenAPI specs).
Key Information: Endpoints, request methods (GET, POST, etc.), headers, query parameters, request body, and response format., Authentication Mechanisum

Setup_Project on Postman
Setup Environment:Create Environment Variables for base URLs, tokens, and other reusable data.
Write Tests:Use Postman’s built-in scripting feature (JavaScript-based)
 Conclusion --> Understand API Automaion requirment and Manual Egde test case 
 
Setup_Project on Automation with Resstassured 
Setup Project:
Write API Tests:
Integrate with CI/CD:


Example from Previous Projects
Scenario: Validating the login functionality of a REST API.
Steps:
Postman:
Created a collection for the Login API.
Tested positive scenarios (valid credentials) and negative scenarios (invalid credentials, missing fields).
Rest Assured:
Automated tests for response codes, token generation, and error messages.
Validated performance using response times.


****************************How do you ensure that your test coverage aligns with the defined acceptance criteria in Agile sprints?  *************************
How do you ensure that your test coverage aligns with the defined acceptance criteria in Agile sprints?
1.Collaborate with the product owner, developers, and stakeholders to create detailed and unambiguous acceptance criteria.
2. Write Test Cases Based on Acceptance Criteria --> Map test cases to criteria
3. Implement a Traceability Matrix
4. Regularly sync with the team to review progress and ensure testing efforts stay aligned with evolving criteria.
6. Reviews of Test Cases with BA and Stakeholder
7. Leverage Testing Tools --> Use tools like JIRA, or Zephyr to document and link acceptance criteria to test cases, enabling visibility and alignment.

****************************** Key challeges face During Manual Testing *********************
Data Management: Handling and preparing test data is complex.
Requirement : when dealing with large datasets or frequent changes
Environment Issues: Environment in not satbel or fcaing performance issue and slowness 
Defect Reproduction: Intermittent issues are hard to reproduce and taking longer time to perform End to End Test 
Limited Test Coverage: Testers may not be able to cover all possible test scenarios, especially edge cases, Some defects may remain undetected, impacting product quality.


***************** Key Indicators That Manual Testing Is Ready for Automation  ************************
Manual test execution is complete and successful for the current scope.
Repetitive tasks are taking up significant manual effort.
Critical defects have been detected, resolved, and validated.
Test cases requiring regression or cross-environment testing are identified.
A stable application and environment are available.
The team has the necessary skills and tools for automation.

************** What key metrics do you use to measure the success of automation testing? ***************
Answer:
Automation Coverage
Execution Time
Defect Detection Rate
Flakiness Rate
Savings in time and cost compared to manual testing.
Pass/Fail Ratio

************************  Resolve a Complex Testing Issue API ***********************

  While testing a new feature for a web application—an interactive dashboard for analytics—I discovered
  
  Isusue -> dashboard occasionally failed to load specific user data when filters were applied.
            Taking longer time to load
			No content Avilable shows the messages , (Even i check on Database  data is there )
			It happend with respect to User as well 
			
 Why Happend -> Reviewed the logs and identified that the issue was related to API requests timing out when filters were applied.
                When user is onboard on application and select the template with respect to type of loan missing some parameters ,some to the field is pass nulll value on HTTP request body
 
			Steps Taken: 
			Collaboration with Cross-Functional Teams: Developers ,Product Managers,UX Designers,DevOps 
			Proposed and Tested Solutions: 
			Developers -> optimized the database query, adding appropriate indexes and caching mechanisms., add validation with respect to template and redmark field is manditory
			Tester ->  a) updated the test cases to include edge cases and scenarios with large datasets to ensure the issue would not recur.
			           b) Conducted regression testing to verify the fix did not introduce new issues. 
					      Collaborated with the QA team to run exploratory tests on the entire dashboard functionality.
		   Post-Deployment Monitoring -> Used monitoring tools to track API performance and gather feedback from end users.
		   
		   
		   Achive -> 
		   A 40% improvement in dashboard load times.
           Higher user satisfaction due to consistent and accurate filtering.
           Improved collaboration and streamlined communication processes between teams for future projects.
		   
		   
************************ Resolving a Complex Testing Issue Using JMeter ****************************
              While testing the performance of a new REST API for a financial services application,	
 
   Issue --> encountered intermittent slowdowns under high user loads 
     
	  How to Solve -> 
	 Apache JMeter to simulate real-world traffic :
	 Added a Thread Group with 500 virtual users, ramping up over 10 seconds with Constant times
	 Included Dynamic Parameters 
     Configured Assertions to validate response time and status codes.
	 using View Results Tree and Summary Report listeners.
	 
	 Why Happend -> 
	 Observed that response times for some requests exceeded 5 seconds (expected < 1 second).
	 Intermittent 500 Internal Server Errors were logged, but only under high concurrency.
	 
	 Solution --> 
	 Database Bottleneck-> Slow queries caused delays during API responses.
	 Thread Pool Saturation: The application server's thread pool was maxing out under high concurrency.
     Improper Caching: Redundant API calls were hitting the database instead of using cached data.
	 
	 Achive- > 
	 Error rate reduced to nearly 0% during high concurrency.


  ****************************  Read a PDF File Using  Automaion ***********************
  
  A ----> Add Apache PDFBox Dependency 
          <groupId>org.apache.pdfbox</groupId>
		  
  B ----> 
        File file = new File(filePath);
        try (PDDocument document = PDDocument.load(file)) {      if PDF file is encrypted or password Protected ----> PDDocument document = PDDocument.load(file, "password")  
            if (!document.isEncrypted()) {
                   PDFTextStripper pdfStripper = new PDFTextStripper(); -> // Create a PDFTextStripper object to extract text  
                String text = pdfStripper.getText(document); -> // Extract text from the PDF document  
                System.out.println("Extracted Text:"); -> // Print the extracted text
                System.out.println(text);
            } else {
                System.out.println("The PDF file is encrypted.");
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }  
	
	
	<--- Reading Specific Pages --->
	
	           PDFTextStripper pdfStripper = new PDFTextStripper();
              pdfStripper.setStartPage(2);  // Start from page 2
              pdfStripper.setEndPage(3);    // End at page 3
              
              String text = pdfStripper.getText(document);
              System.out.println("Extracted Text from pages 2-3:");
              System.out.println(text);
			  
   <--- image in PDF files --->
   
   PDFRenderer pdfRenderer = new PDFRenderer(document);
   BufferedImage image = pdfRenderer.renderImageWithDPI(0, 300); // Render first page with 300 DPI
   ImageIO.write(image, "PNG", new File("output.png")); /
   
   
   ***************************************************************************************************************
   
     ****************************  What is  API Gateway   ***********************
	     is a software component thatacts as an entry point for client requests to backend services or microservices in a distributed system.
	     It serves as a single entry point for all clients (such as web, mobile, or IoT devices) and routes their requests to the appropriate backend services.
	     Key Functions of an API Gateway:-> Routing , Load Balancing , Request/Response Transformation , Authentication & Authorization
	 	 
	 **************  Handling Tight Deadlines  ******************************
	 --{<|>}[>  our team is given a short timeframe to deliver a new feature.
	 
	Prioritize Testing:Focus on critical paths, such as login, trade execution, and account management.
    Risk-Based Testing:Identify high-risk areas and allocate more resources there.
	Leverage Automation:Use existing automation scripts for regression.
    Communicate Clearly:Keep stakeholders updated on what can be tested within the timeline and any potential risks.
                        **************    ******************************




****************   Microservice <---> Kafka <----> API Gatway Testing  *********************
Produce Messages to Kafka: Validate that the application correctly produces messages to Kafka topics.
Consume Messages from Kafka: Verify that the microservices correctly consume and process messages from Kafka topics.

API Gateway Integration:
Verify that the API Gateway routes requests to the correct microservices.
Validate the end-to-end functionality by testing APIs exposed via the API Gateway.

Data Flow: Ensure data consistency and correctness as it flows between the API Gateway, Kafka, and microservices.

Error Handling:
Test invalid Kafka messages.
Test API Gateway behavior for invalid or unauthenticated requests.

Performance:
Validate message throughput and latency in Kafka.
Test the API Gateway's ability to handle a high volume of concurrent requests.













